# Assignment Feedback: Week 4: Dimensionality Reduction

**Student:** haydenorenstein
**Total Score:** 25/40 (62.5%)

**Grade Category:** D (Poor)

---

## Problem Breakdown

### Exercise 1 (6/16 = 37.5%)

**Part pipeline-part1** (pipeline-part1.code): 0/0 points

_Feedback:_ You correctly applied PCA and visualized a 2D scatter. However, for reconstruction you created a new PCA instance for inverse_transform—use the same fitted PCA. Also, plotting only the first two reconstructed pixel values isn’t meaningful; visualize images (imshow) to show approx

**Part pipeline-part2** (pipeline-part2.code): 1/4 points

_Feedback:_ You performed PCA correctly and produced a scree plot, but the task required reducing to 2 components and showing a 2D scatter colored by class. No 2D projection was plotted and classes weren’t visualized. Update to PCA(n_components=2) and scatter by y.

**Part pipeline-part3** (pipeline-part3.code): 1/4 points

_Feedback:_ You fit PCA and used explained_variance_ratio_, but you didn’t produce the required scree plot. Plot the first 40 components’ percent variance explained on the y-axis. E.g., use explained_variance_ratio_[:40], multiply by 100, and plt.plot with proper labels/ticks.

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ You correctly used the previously computed result (148 components) to select the number of components for 95% variance and applied it. While this cell doesn’t recompute n_components, it’s consistent with your prior work. Nicely done.

**Part pipeline-part5** (pipeline-part5.code): 0/4 points

_Feedback:_ Your code performs k-NN classification with/without PCA, but does not address Step 5. You needed to use the Step 4 dimensionality (148) to reduce a single digit, inverse_transform it, and visualize with plot_mnist_digit. No digit reconstruction/plot was provided.

---

### Exercise 2 (10/10 = 100.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Correct: you apply t-SNE to MNIST with 2D output and plot colored by labels. This accomplishes the goal. Optional improvements: add a colorbar, tweak point size for visibility, and consider tuning perplexity/learning_rate or subsampling for speed/clarity.

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good TSNE -> KNN pipeline and you report accuracy. Combining train+test before TSNE matches the common approach here (as in the reference). Solid implementation.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Correct: you compute accuracy with KNN on learned embeddings. Fitting UMAP on train and transforming test is proper. KNN usage and accuracy calculation are correct. Nice work.

---

### Exercise 4 (9/14 = 64.3%)

**Part ex2-part1** (ex2-part1.code): 0/0 points

_Feedback:_ You applied PCA (found 95% comp count, then used 2D), visualized, and evaluated KNN—good. However, you didn’t try UMAP, vary PCA dimensionality systematically, or compare KNN across dims/UMAP params as requested. Add UMAP with different n_neighbors/min_dist and plot.

**Part ex2-part2** (ex2-part2.code): 3/7 points

_Feedback:_ You built a valid DR+KNN pipeline with proper train/test separation and a clear plot, but you used UMAP instead of PCA as required and did not leverage your prior PCA work. To earn full credit, apply PCA (fit on train, transform test) before KNN and report accuracy.

**Part ex2-part3** (ex2-part3.answer): 6/7 points

_Feedback:_ Good insight: UMAP can outperform PCA on non-linear structure; PCA is linear. However, you didn’t reference or compare the actual accuracies you computed (UMAP vs PCA vs original), and the last sentence is unclear. Cite your results to support claims.

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-10-27 18:51:26 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*